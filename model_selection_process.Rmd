---
title: "ECON5182 Final Project"
author: "LI, Ruifeng; SHI, Yifan; WANG, Yanqi"
date: "2023/5/18"
output: html_document
---

```{r setup, include=FALSE}
if (!requireNamespace("knitr", quietly = TRUE)) {
  install.packages("knitr")
}
knitr::opts_chunk$set(echo = TRUE)
```

## Load data into R

```{r}
load(url("https://github.com/zhentaoshi/Econ5821/raw/main/data_example/dataset_inf.Rdata"))
```

## Built-in Evaluation Function

Our forecast performance is measured by the out-of-sample R-squared and Mean Square Error.

```{r}
oos_rsq <- function(y_true, y_pred) {
  numerator <- var(y_true - y_pred)
  denominator <- var(y_true)
  oos_r_squared <- 1 - (numerator / denominator)
  return(oos_r_squared)
}

oos_mse <- function(y_true, y_pred){
  mse <- mean((y_true-y_pred)^2)
  return(mse)
}
```

## Data Preparation

first we...

```{r}
# Convert X to a data frame
X_df <- as.data.frame(X)

# Function to create lagged variables
lag_columns <- function(data, lags) {
  data_lagged <- data
  for (i in 2:ncol(data)) {
    for (lag in 1:lags) {
      lagged_colname <- paste(colnames(data)[i], paste0("lag", lag), sep = "_")
      data_lagged[, lagged_colname] <- c(rep(NA, lag), data[-((nrow(data) - lag + 1):nrow(data)), i])
    }
  }
  return(data_lagged)
}

# Create lagged variables for X_df
lags <- 3 # Change this to the desired number of lags
X_df_lagged <- lag_columns(X_df, lags)

# Remove variables at time t (non-lagged variables)
X_df_lagged_only <- subset(X_df_lagged, select = -(2:ncol(X_df)))
# Remove rows with missing values
X_df_lagged_clean <- na.omit(X_df_lagged_only)

# Function to normalize columns using min-max normalization
min_max_normalize_columns <- function(data) {
  for (i in 2:ncol(data)) {
    min_value <- min(data[, i], na.rm = TRUE)
    max_value <- max(data[, i], na.rm = TRUE)
    data[, i] <- (data[, i] - min_value) / (max_value - min_value)
  }
  return(data)
}

# Apply min-max normalization to X_df_lagged_clean
X_df_lagged_normalized <- min_max_normalize_columns(X_df_lagged_clean)

```

Second we split the data into training set and test set
```{r}
# Split the outcome (CPI&PPI)
cpi_vec <- cpi$CPI[-(1:lags)]  # remove the first 'lags' observations to align with X_df_lagged_clean
ppi_vec <- ppi$PPI[-(1:lags)]

# Determine the split point for training and testing sets
n <- length(cpi_vec)
train_ratio <- 0.8
train_size <- floor(train_ratio * n)

cpi_train <- cpi_vec[1:train_size]
cpi_test <- cpi_vec[(train_size + 1):n]
ppi_train <- ppi_vec[1:train_size]
ppi_test <- ppi_vec[(train_size + 1):n]

# For original data of X
X_mat <- as.matrix(X_df_lagged_clean)
X_train <- X_mat[1:train_size, -1]
X_test <- X_mat[(train_size + 1):n, -1]

# For normalized data of X
X_mat_norm <- as.matrix(X_df_lagged_normalized)
X_train_norm <- X_mat_norm[1:train_size, -1]
X_test_norm <- X_mat_norm[(train_size + 1):n, -1]

```

## Linear model 1: Elastic Net with K-fold Cross Validation
```{r}
if (!requireNamespace("glmnet", quietly = TRUE)) install.packages("glmnet")
library(glmnet)
set.seed(11)
a=seq(0.01,0.99,0.01)

#CPI
elnet.cv_cpi<-c()
for (i in a) {
  set.seed(123)
  cv.out<-cv.glmnet(X_train,cpi_train,alpha=i)
  elnet.cv_cpi<-rbind(elnet.cv_cpi,data.frame(cvm = cv.out$cvm[cv.out$lambda == cv.out$lambda.min], 
                                              lambda.min = cv.out$lambda.min, alpha = i))
}

plot(1:nrow(elnet.cv_cpi),elnet.cv_cpi[,1],xlab="alpha and corresponding best lambda for cpi",ylab="cv mean square error",type="b")
cpi_optcv <- which.min(elnet.cv_cpi$cvm)  #find the optimal alpha and lambda
points(cpi_optcv,elnet.cv_cpi[cpi_optcv,1], col="red",cex=2,pch=20)

elnet.cpi<-glmnet(X_test,cpi_test,alpha=elnet.cv_cpi[cpi_optcv,3],lambda=elnet.cv_cpi[cpi_optcv,2])

coef_names_cpi <- colnames(X_train)[which(coef(elnet.cpi)[-1,] != 0)]
mse_cpi_kfoldnet<- oos_mse(cpi_test,predict(elnet.cpi,newx=X_test))
rsq_cpi_kfoldnet <- oos_rsq(cpi_test,predict(elnet.cpi,newx=X_test))

#PPI#
elnet.cv_ppi<-c()
for (i in a) {
  set.seed(1234)
  cv.out<-cv.glmnet(X_train,ppi_train,alpha=i)
  elnet.cv_ppi<-rbind(elnet.cv_ppi,data.frame(cvm = cv.out$cvm[cv.out$lambda == cv.out$lambda.min], 
                                              lambda.min = cv.out$lambda.min, alpha = i))
}

plot(1:nrow(elnet.cv_ppi),elnet.cv_ppi[,1],xlab="alpha and corresponding best lambda for ppi",ylab="cv mean square error",type="b")
ppi_optcv <- which.min(elnet.cv_ppi$cvm)  #find the optimal alpha and lambda
points(ppi_optcv,elnet.cv_ppi[ppi_optcv,1], col="red",cex=2,pch=20)

elnet.ppi<-glmnet(X_test,ppi_test,alpha=elnet.cv_ppi[ppi_optcv,3],lambda=elnet.cv_ppi[ppi_optcv,2])

coef_names_ppi <- colnames(X_train)[which(coef(elnet.ppi)[-1,] != 0)]
mse_ppi_kfoldnet<-oos_mse(ppi_test,predict(elnet.ppi,newx=X_test))
rsq_ppi_kfoldnet <- oos_rsq(ppi_test,predict(elnet.ppi,newx=X_test))

coef_names_cpi
coef_names_ppi
cat("Mean Squared Error and R^2 for CPI model:", mse_cpi_kfoldnet, "and ", rsq_cpi_kfoldnet,"\n")
cat("Mean Squared Error and R^2 for PPI model:", mse_ppi_kfoldnet, "and ", rsq_ppi_kfoldnet, "\n")

```

## Elastic Net with Nested Cross Validation
```{r}
#Elastic Net with Nested CV#####
library(glmnet)
library(dplyr)

compute_error <- function(x_train,y_train,x_test,y_test,alpha, lambda) {
  model <- glmnet(x_train, y_train, alpha = alpha, lambda = lambda)
  predicted_values <- predict(model, x_test)
  return(mean((y_test - predicted_values)^2))  
}

# Nested CV
nested_cv <- function(x, y, k = 10) {
  n <- nrow(x)
  alpha_seq <- seq(0.01, 0.99, 0.01)
  cv_results <- data.frame()
  
  for (i in 1:k) {
    train_size <- floor(i * n / k)
    test_size <- floor(n / k)
    x_train <- as.matrix(x[1:(train_size), ])
    y_train <- unlist(y[1:(train_size)])
    x_test <- as.matrix(x[(train_size + 1):(train_size + test_size), ])
    y_test <- unlist(y[(train_size + 1):(train_size + test_size)])
    
    cv_errors <- numeric(length(alpha_seq))
    lambda.min <- numeric(length(alpha_seq))
    for (j in seq_along(alpha_seq)) {
      alpha <- alpha_seq[j]
      cv.out <- cv.glmnet(x_train, y_train, alpha = alpha)
      lambda.min[j] <- cv.out$lambda.min
      cv_errors[j] <- compute_error(x_train,y_train,x_test,y_test, alpha, lambda.min)
    }
    
    cv_results <- rbind(cv_results, data.frame(alpha = alpha_seq, cv_error = cv_errors, lambda.min=lambda.min))
  }
  
  return(cv_results)
}


x_nest <- X_df_lagged_clean[1:train_size, -1]

#cpi
cpi_nest_results <- nested_cv(x_nest, cpi_train)

# Find the optimal alpha and corresponding lambda
cv_means_cpi <- aggregate(cv_error ~ alpha, cpi_nest_results, FUN = mean)
best_alpha_cpi <- cv_means_cpi$alpha[which.min(cv_means_cpi$cv_error)]
best_cv_cpi <- subset(cpi_nest_results, alpha == best_alpha_cpi)
best_lambda_cpi <- best_cv_cpi$lambda.min[which.min(best_cv_cpi$cv_error)]

fit.cpi.nest <-glmnet(X_test,cpi_test,alpha=best_alpha_cpi,lambda=best_lambda_cpi)
coef.cpi.nest <- colnames(X_train)[which(coef(fit.cpi.nest)[-1,] != 0)]
mse_cpi_nestelnet <- oos_mse(cpi_test,predict(fit.cpi.nest,newx=X_test))
rsq_cpi_nestelnet <- oos_rsq(cpi_test,predict(fit.cpi.nest,newx=X_test))


#ppi
ppi_nest_results <- nested_cv(x_nest, ppi_train)
# Find the optimal alpha and corresponding lambda 
cv_means_ppi <- aggregate(cv_error ~ alpha, ppi_nest_results, FUN = mean)
best_alpha_ppi <- cv_means_ppi$alpha[which.min(cv_means_ppi$cv_error)]
best_cv_ppi <- subset(ppi_nest_results, alpha == best_alpha_ppi)
best_lambda_ppi <- best_cv_ppi$lambda.min[which.min(best_cv_ppi$cv_error)]

fit.ppi.nest <-glmnet(X_test,ppi_test,alpha=best_alpha_ppi,lambda=best_lambda_ppi)
coef.ppi.nest <- colnames(X_train)[which(coef(fit.ppi.nest)[-1,] != 0)]
mse_ppi_nestelnet <- oos_mse(ppi_test,predict(fit.ppi.nest,newx=X_test))
rsq_ppi_nestelnet <- oos_rsq(ppi_test,predict(fit.ppi.nest,newx=X_test))

cat("Mean Squared Error and R^2 for CPI Elastic net(Nested CV) model:", mse_cpi_nestelnet, "and ", rsq_cpi_nestelnet,"\n")
cat("Mean Squared Error and R^2 for PPI Elastic net(Nested CV) model:", mse_ppi_nestelnet, "and ", rsq_ppi_nestelnet, "\n")




```

## Feature Selection

```{r}
# first step, Pre-selection of variables using ols model
selected_vars_cpi <- c()
for (i in 1:ncol(X_train_norm)) {
  ols_model_cpi <- lm(cpi_train ~ X_train_norm[, i])
  p_value_cpi <- summary(ols_model_cpi)$coefficients[2, 4]
  
  if (p_value_cpi < 0.05) {
    selected_vars_cpi <- c(selected_vars_cpi, colnames(X_train_norm)[i])
  }
}

selected_vars_ppi <- c()
for (i in 1:ncol(X_train_norm)) {
  ols_model_ppi <- lm(ppi_train ~ X_train_norm[, i])
  p_value_ppi <- summary(ols_model_ppi)$coefficients[2, 4]
  
  if (p_value_ppi < 0.05) {
    selected_vars_ppi <- c(selected_vars_ppi, colnames(X_train_norm)[i])
  }
}
# second step, where we choose the most relevant variables using LASSO and Random Forest approaches:
# Install and load the required packages
if (!requireNamespace("randomForest", quietly = TRUE)) install.packages("randomForest")
library(glmnet)
library(randomForest)

# Function to extract top variable names based on their importance
top_variables <- function(importance, n_top) {
  sorted_importance <- importance[order(importance, decreasing = TRUE)]
  names(sorted_importance)[1:n_top]
}

# LASSO for CPI
X_train_selected_cpi <- X_train_norm[, selected_vars_cpi]
X_test_selected_cpi <- X_test_norm[, selected_vars_cpi]

lasso_model_cpi <- glmnet(X_train_selected_cpi, cpi_train, alpha = 1)
best_lambda_cpi <- lasso_model_cpi$lambda.min
lasso_coefs_sparse_cpi <- predict(lasso_model_cpi, type = "coefficients", s = best_lambda_cpi)[-1, , drop = FALSE]

# Convert the sparse matrix to a dense matrix
lasso_coefs_dense_cpi <- as.matrix(lasso_coefs_sparse_cpi)

# Extract the top variables
lasso_top_vars_cpi <- top_variables(lasso_coefs_dense_cpi[, 1], 15)  # Make sure to select the first column from lasso_coefs_dense
print(lasso_top_vars_cpi)

# LASSO for PPI
X_train_selected_ppi <- X_train_norm[, selected_vars_ppi]
X_test_selected_ppi <- X_test_norm[, selected_vars_ppi]

lasso_model_ppi <- glmnet(X_train_selected_ppi, ppi_train, alpha = 1,)
best_lambda_ppi <- lasso_model_ppi$lambda.min
lasso_coefs_sparse_ppi <- predict(lasso_model_ppi, type = "coefficients", s = best_lambda_ppi)[-1, , drop = FALSE]

# Convert the sparse matrix to a dense matrix
lasso_coefs_dense_ppi <- as.matrix(lasso_coefs_sparse_ppi)

# Extract the top variables
lasso_top_vars_ppi <- top_variables(lasso_coefs_dense_ppi[, 1], 10)  # Make sure to select the first column from lasso_coefs_dense
print(lasso_top_vars_ppi)

# Random Forest for CPI
rf_model_cpi <- randomForest(X_train_selected_cpi, cpi_train, importance = TRUE)
rf_importance_cpi <- rf_model_cpi$importance[, "IncNodePurity"]  # Retrieve IncNodePurity as the importance values

rf_top_vars_cpi <- top_variables(rf_importance_cpi, 15)
print(rf_top_vars_cpi)

# Combine the top variables from LASSO and Random Forest
top_vars_comb_cpi <- unique(c(lasso_top_vars_cpi, rf_top_vars_cpi))

# Random Forest for PPI
rf_model_ppi <- randomForest(X_train_selected_ppi, ppi_train, importance = TRUE)
rf_importance_ppi <- rf_model_ppi$importance[, "IncNodePurity"]  # Retrieve IncNodePurity as the importance values

rf_top_vars_ppi <- top_variables(rf_importance_ppi, 10)
print(rf_top_vars_ppi)

# Combine the top variables from LASSO and Random Forest
top_vars_comb_ppi <- unique(c(lasso_top_vars_ppi, rf_top_vars_ppi))

# Prepare the selected data
X_train_final_cpi <- X_train_norm[, top_vars_comb_cpi]
X_train_final_ppi <- X_train_norm[, top_vars_comb_ppi]

X_test_final_cpi <- X_test_norm[, top_vars_comb_cpi]
X_test_final_ppi <- X_test_norm[, top_vars_comb_ppi]

```

## Tree model 1: Random Forest

```{r}
# Load required libraries
if (!requireNamespace("forecast", quietly = TRUE)) install.packages("forecast")
if (!requireNamespace("randomForest", quietly = TRUE)) install.packages("randomForest")

library(forecast)
library(randomForest)

# Time series cross-validation with a growing window
time_series_slices <- createTimeSlices(1:length(cpi_train),
                                       initialWindow = floor(length(cpi_train) / 2),
                                       horizon = 1,
                                       fixedWindow = FALSE)

time_series_cv <- trainControl(method = "timeslice",
                               index = time_series_slices$train,
                               indexOut = time_series_slices$test)

# Part 1: Random Forest model
# Hyperparameter tuning for RF model
rf_tune_grid <- expand.grid(mtry = c(1, 2, 3, 4, 5))

# Tuning RF model for CPI
rf_tuned_cpi <- train(cpi_train ~ ., data = data.frame(cpi_train, X_train_final_cpi),
                      method = "rf",
                      trControl = time_series_cv,
                      tuneGrid = rf_tune_grid)

# Tuning RF model for PPI
rf_tuned_ppi <- train(ppi_train ~ ., data = data.frame(ppi_train, X_train_final_ppi),
                      method = "rf",
                      trControl = time_series_cv,
                      tuneGrid = rf_tune_grid)

# Fit the best RF models, make predictions, and calculate OOS MSE and R-squared
best_rf_cpi <- rf_tuned_cpi$finalModel
cpi_rf_pred <- predict(best_rf_cpi, X_test_final_cpi)
mse_cpi_rf <- oos_mse(cpi_test, cpi_rf_pred)
rsq_cpi_rf <- oos_rsq(cpi_test, cpi_rf_pred)

best_rf_ppi <- rf_tuned_ppi$finalModel
ppi_rf_pred <- predict(best_rf_ppi, X_test_final_ppi)
mse_ppi_rf <- oos_mse(ppi_test, ppi_rf_pred)
rsq_ppi_rf <- oos_rsq(ppi_test, ppi_rf_pred)

cat("CPI Random Forest MSE:", mse_cpi_rf, "R-squared:", rsq_cpi_rf, "\n")
cat("PPI Random Forest MSE:", mse_ppi_rf, "R-squared:", rsq_ppi_rf, "\n")
```

## Tree model 1: XGBoosting
```{r}

if (!requireNamespace("xgboost", quietly = TRUE)) install.packages("xgboost")
library(tidyverse)
library(caret)
library(xgboost)

# Parameter tuning
tuneGrid <- expand.grid(nrounds = c(50, 100, 150),
                        max_depth = c(3, 5, 7),
                        eta = c(0.01, 0.1, 0.3),
                        gamma = c(0, 1, 3),
                        colsample_bytree = c(0.6, 0.8, 1),
                        min_child_weight = c(1, 3, 5),
                        subsample = c(0.6, 0.8, 1))

xgbControl <- trainControl(method = "cv", number = 10)

xgb_cpi_tuned <- train(x = X_train_final_cpi, y = cpi_train,
                       method = "xgbTree",
                       trControl = xgbControl,
                       tuneGrid = tuneGrid)

xgb_ppi_tuned <- train(x = X_train_final_ppi, y = ppi_train,
                       method = "xgbTree",
                       trControl = xgbControl,
                       tuneGrid = tuneGrid)

# Train the final models with the best parameters
xgb_cpi_final <- xgboost(data = as.matrix(X_train_final_cpi), label = cpi_train,
                         nrounds = xgb_cpi_tuned$bestTune$nrounds,
                         max_depth = xgb_cpi_tuned$bestTune$max_depth,
                         eta = xgb_cpi_tuned$bestTune$eta,
                         gamma = xgb_cpi_tuned$bestTune$gamma,
                         colsample_bytree = xgb_cpi_tuned$bestTune$colsample_bytree,
                         min_child_weight = xgb_cpi_tuned$bestTune$min_child_weight,
                         subsample = xgb_cpi_tuned$bestTune$subsample)

xgb_ppi_final <- xgboost(data = as.matrix(X_train_final_ppi), label = ppi_train,
                         nrounds = xgb_ppi_tuned$bestTune$nrounds,
                         max_depth = xgb_ppi_tuned$bestTune$max_depth,
                         eta = xgb_ppi_tuned$bestTune$eta,
                         gamma = xgb_ppi_tuned$bestTune$gamma,
                         colsample_bytree = xgb_ppi_tuned$bestTune$colsample_bytree,
                         min_child_weight = xgb_ppi_tuned$bestTune$min_child_weight,
                         subsample = xgb_ppi_tuned$bestTune$subsample)

# Evaluate the final models on the validation set
xgb_cpi_preds <- predict(xgb_cpi_final, as.matrix(X_test_final_cpi))
xgb_ppi_preds <- predict(xgb_ppi_final, as.matrix(X_test_final_ppi))

##### Calculate out-of-sample R-squared####


rsq_cpi_xgb <- oos_rsq(cpi_test,xgb_cpi_preds)
mse_cpi_xgb <- oos_mse(cpi_test,xgb_cpi_preds)

rsq_ppi_xgb <- oos_rsq(ppi_test,xgb_ppi_preds)
mse_ppi_xgb <- oos_mse(ppi_test,xgb_ppi_preds)



```

###### Compare the best model for CPI and PPI


```{r}
report1_results_cpi <- list()
report1_results_cpi[["Net_cpi_kfold"]] <- list(oos_mse = mse_cpi_kfoldnet, oos_rsq = rsq_cpi_kfoldnet)
report1_results_cpi[["Net_cpi_nestelnet"]] <- list(oos_mse = mse_cpi_nestelnet, oos_rsq = rsq_cpi_nestelnet)

report1_results_cpi[["rf_cpi"]] <- list(oos_mse = mse_cpi_rf, oos_rsq = rsq_cpi_rf)
report1_results_cpi[["XGB_cpi"]] <- list(oos_mse = mse_cpi_xgb, oos_rsq = rsq_cpi_xgb)
```


```{r}
report1_results_ppi <- list()
report1_results_ppi[["Net_cpi_kfold"]] <- list(oos_mse = mse_ppi_kfoldnet, oos_rsq = rsq_ppi_kfoldnet)
report1_results_ppi[["Net_cpi_nestelnet"]] <- list(oos_mse = mse_ppi_nestelnet, oos_rsq = rsq_ppi_nestelnet)

report1_results_ppi[["rf_cpi"]] <- list(oos_mse = mse_ppi_rf, oos_rsq  = rsq_ppi_rf)
report1_results_ppi[["XGB_cpi"]] <- list(oos_mse = mse_ppi_xgb, oos_rsq = rsq_ppi_xgb)
```


```{r}
print(report1_results_cpi)
print(report1_results_ppi)

```
